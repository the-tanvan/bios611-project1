```{r include=FALSE}
library(gbm)
library(tidyverse)
library(caret)
library(ggfortify)
```
# Q1

Repeat your GBM model. Contrast your results with the results for the previous exercise.

With the gbm modeling below, it was found that this model with an accuracy of 91.8% is more accurate than the previous model with an accuracy of 49.8%.

```{r}
q1 = read.csv("/home/rstudio/hw5/q1.csv")
q1.analysis=q1%>%mutate(Male=(Gender=="Male"))
set.seed(392)
q1.train=sample(c(TRUE,FALSE),nrow(q1.analysis),rep=TRUE)
q1.test=(!q1.train)
q1.train = q1.analysis[q1.train,]
q1.test = q1.analysis[q1.test,]
gbm_model <- gbm(formula = Male ~ Weight+Height, 
                    distribution = "bernoulli", 
                    data = q1.train,
                    n.trees = 1000)
gbm_predict = (predict(gbm_model, newdata=q1.test, n.trees=1000,type="response")>0.5)
gbm_results=as.data.frame(cbind(Truth=q1.test$Male,Prediction=gbm_predict)) %>% mutate(correct=(Truth==Prediction))
table(gbm_results$correct)
4585/(4585+408)
```

# Q2
1.  Examine the dataset for any irregularities. Make the case for filtering out a subset of rows (or for not doing so).

```{r}
q2 = read.csv("/home/rstudio/hw5/q2.csv")

ggplot(data=q2, aes(x=Total)) + geom_histogram(bins=30)
lowtotal=q2%>%filter(Total==5)
nrow(lowtotal)
nrow(lowtotal)/nrow(q2)

q2_filtered=q2%>%filter(Total!=5)

```
This dataset contained 177 participants with a total of 5. This subset is about a third of the dataset, but many of these subjects have the exact same values for each skill. Therefore, I will exclude them from the dataset to remove the bias they would add.

2.  Perform a principal component analysis on the numerical columns of this data. How many components do we need to get 85% of the variation in the data set?

```{r}
q2_analysis=q2_filtered%>%select(-Name,-Alignment)
q2_pca=prcomp(q2_analysis,scale=T)
summary(q2_pca)
```
To capture 85% of the variance, we need  4 components.

3.  Do we need to normalize these columns or not?

Due to the variation in scale of each of the predictors, we do need to normalize the columns.

4.  Is the "total" column really the total of the values in the other columns?

```{r}
q2_compare = q2 %>% mutate(true_total=Intelligence+Strength+Speed+Durability+Power+Combat)
all(q2_compare$true_total==q2$Total)
```
We found that the total column is truly the total of the values in the other columns.

5.  Should we have included in in the PCA? What do you expect about the largest principal components and the total column? Remember, a given principal component corresponds to a weighted combination of the original variables.

We should not have included the total column because it is already a weighted combinations of the other variables and is simply adding unnecessary information.

6.  Make a plot of the two largest components. Any insights?

```{r}
autoplot(q2_pca, x = 1, y = 2)
```
# Q3

Use Python/sklearn to perform a TSNE dimensionality reduction.

See hw5.ipynb for the TSNE dimensionality reduction.

In R, plot the results. Color each point by the alignment of the associated character. Any insights?

```{r}
q3=read.csv("/home/rstudio/hw5/TSNE.csv",header=F)
V3=q2$Alignment
q3_graph=cbind(q3,V3)
head(q3_graph)
ggplot(data=q3_graph,aes(x=V1,y=V2))+geom_point(aes(color=V3))
```

It seems that neutral characters can be found in the upper band but not in the lower cluster. Overall, it seems the alignments are pretty mixed between the two clusters.

# Q4

See hw5.ipynb.

# Q5

Using the Caret library, train a GBM model which attempts to predict character alignment. What are the final parameters that caret determines are best for the model.

Hints: you want to use the "train" method with the "gbm" method. Use "repeatedcv" for the characterization method. If this is confusing, don't forget to read the Caret docs.

```{r}

```

# Q6

A conceptual question: why do we need to characterize our models using strategies like k-fold cross validation? Why can't we just report a single number for the accuracy of our model?

To determine the accuracy of a model, we need to use our data to train and test it. If we do a simple split of train and testing data, we may split our data in a way that is biased and leads to a poor model. By using a strategy like k-fold cross validation, we train our model on multiple randomly selected samples from our data, reducing biased and allowing us to have a better estimate of the accuracy of our model.

# Q7

Describe in words the process of recursive feature elimination. 

Recursive feature elimination is a way to select the best predictors for a model. It begins by creating a model with all of the predictors available and then removing the least important predictors. This process is repeated over and over again until the process reaches a stopping point indicated by the analyst.